import csv
import re
from urllib.parse import unquote, parse_qs, urlparse

INPUT_FILE = "northern_stores_complete.csv"
OUTPUT_FILE = "northern_stores_cleaned.csv"

def clean_phone(phone_str):
    """Standardizes phone numbers to (XXX) XXX-XXXX."""
    if not phone_str or phone_str.lower() in ["n/a", ""]:
        return "N/A"
    digits = re.sub(r'\D', '', phone_str)
    if len(digits) == 10:
        return f"({digits[:3]}) {digits[3:6]}-{digits[6:]}"
    if len(digits) == 11 and digits.startswith('1'):
        return f"({digits[1:4]}) {digits[4:7]}-{digits[7:]}"
    return "N/A"

def clean_url(url):
    """Extracts the real URL from YellowPages redirects."""
    if not url or url.lower() in ["n/a", ""]:
        return "N/A"
    
    # Check if it's a YellowPages redirect
    if "yellowpages.ca/gourl" in url:
        try:
            parsed = urlparse(url)
            query = parse_qs(parsed.query)
            if 'redirect' in query:
                # Get the actual URL
                real_url = query['redirect'][0]
                return real_url
        except:
            return url
            
    return url

def title_case_address(addr):
    """Properly formats address casing (e.g., 'parry sound' -> 'Parry Sound')."""
    if not addr or addr.lower() in ["n/a", ""]:
        return "N/A"
    
    # Capitalize words
    addr = addr.title()
    # Fix 'On' to 'ON' for province
    addr = re.sub(r'\bOn\b', 'ON', addr)
    # Fix postal codes (L1l 1L1 -> L1L 1L1) - Pattern: LetterDigitLetter
    addr = re.sub(r'([A-Za-z]\d[A-Za-z])\s?(\d[A-Za-z]\d)', 
                  lambda m: f"{m.group(1).upper()} {m.group(2).upper()}", addr)
    return addr

def extract_city(address):
    """Extracts city for sorting purposes."""
    if not address or address == "N/A": return "ZZZ"
    # match City, ON
    match = re.search(r'([^,]+),\s*ON', address, re.IGNORECASE)
    if match:
        return match.group(1).strip().upper()
    return "ZZZ"

def process_csv():
    print(f"Reading {INPUT_FILE}...")
    
    try:
        with open(INPUT_FILE, 'r', encoding='utf-8', errors='ignore') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
    except FileNotFoundError:
        print(f"Error: Could not find {INPUT_FILE}")
        return

    cleaned_data = []
    seen_phones = set()
    seen_addresses = set()
    
    # 1. First Pass: Cleaning
    for row in rows:
        name = row.get('Name', '').strip()
        raw_addr = row.get('Address', '').strip()
        raw_phone = row.get('Phone', '').strip()
        raw_web = row.get('Website', '').strip()
        
        # Skip empty rows
        if not name: continue

        # Cleaning
        clean_addr = title_case_address(raw_addr)
        clean_ph = clean_phone(raw_phone)
        clean_wb = clean_url(raw_web)
        
        # Calculate City for sorting
        city = extract_city(clean_addr)
        
        cleaned_data.append({
            "Name": name,
            "Address": clean_addr,
            "Phone": clean_ph,
            "Website": clean_wb,
            "_city": city, # Temporary helper for sorting
            "_has_phone": 1 if clean_ph != "N/A" else 0 # Priority
        })

    # 2. Sort Data
    # Sort by: Has Phone (Desc), City (Asc), Name (Asc)
    # This ensures that if we have duplicates, the one WITH the phone number comes first
    cleaned_data.sort(key=lambda x: (-x['_has_phone'], x['_city'], x['Name']))

    # 3. Deduplication
    final_rows = []
    
    print("Deduplicating...")
    for row in cleaned_data:
        # Strategy 1: Check Phone Number (if exists)
        if row['Phone'] != "N/A":
            if row['Phone'] in seen_phones:
                continue # Skip duplicate phone
            seen_phones.add(row['Phone'])
            
        # Strategy 2: Check Address + Name Similarity (if no phone or unique phone)
        # We use a simplified address key (first 10 chars) to catch slight variations
        addr_key = row['Address'][:15].lower() if row['Address'] != "N/A" else "no_addr"
        
        # If we have this address already, we might want to skip, UNLESS the name is very different
        # For simplicity in this script, we assume one business per address
        if addr_key != "no_addr" and addr_key in seen_addresses:
             # Check if we already have this address
             continue
        
        if addr_key != "no_addr":
            seen_addresses.add(addr_key)
            
        # Remove temporary keys before saving
        final_row = {k: v for k, v in row.items() if not k.startswith('_')}
        final_rows.append(final_row)

    # 4. Write Output
    with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=["Name", "Address", "Phone", "Website"])
        writer.writeheader()
        writer.writerows(final_rows)
        
    print(f"Success! Processed {len(rows)} rows into {len(final_rows)} unique entries.")
    print(f"Saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    process_csv()
